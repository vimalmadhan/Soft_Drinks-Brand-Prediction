{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "485b7481",
   "metadata": {},
   "source": [
    "# Import the necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0392185e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:19:57.805288: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:19:57.805313: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad4999e",
   "metadata": {},
   "source": [
    "# Image Size and Images path for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33be8626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# re-size all the images to this\n",
    "IMAGE_SIZE = [200, 200]\n",
    "\n",
    "train_path = '/home/vimalkumar/Documents/cool drinks'\n",
    "folders = glob('/home/vimalkumar/Documents/cool drinks/*')\n",
    "\n",
    "print(len(folders))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1cf5215",
   "metadata": {},
   "source": [
    "# Using VGG16 model for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4bd05be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-19 13:20:04.149210: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-19 13:20:04.149238: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-19 13:20:04.149252: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (IG154): /proc/driver/nvidia/version does not exist\n",
      "2022-02-19 13:20:04.149424: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "vgg16_model = VGG16(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44fe9da",
   "metadata": {},
   "source": [
    "# Not training the model with existing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0f6dd25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't train existing weights\n",
    "for layer in vgg16_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c732f9",
   "metadata": {},
   "source": [
    "# Flattening is done for converting multi-dimensional array into one dimensional array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df838e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Flatten()(vgg16_model.output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e949907b",
   "metadata": {},
   "source": [
    "# Activation function softmax is used because this model contains more than 2 classes i.e (5 classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c609a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = Dense(len(folders), activation='softmax')(x)\n",
    "\n",
    "# create a model object\n",
    "model = Model(inputs=vgg16_model.input, outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9af56b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.compile(\n",
    "  loss='categorical_crossentropy',\n",
    "  optimizer='adam',\n",
    "  metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6116ba92",
   "metadata": {},
   "source": [
    "# Image Augmentation is done for training Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6f00323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Image Data Generator to import the images from the dataset\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                  validation_split=0.2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d119e0fd",
   "metadata": {},
   "source": [
    "# Splitting the model for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bda96661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3386 images belonging to 5 classes.\n",
      "Found 844 images belonging to 5 classes.\n"
     ]
    }
   ],
   "source": [
    "# Make sure you provide the same target size as initialied for the image size\n",
    "training_set = train_datagen.flow_from_directory(train_path,\n",
    "                                                 target_size = (200, 200),\n",
    "                                                 batch_size = 16,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                subset = 'training',\n",
    "                                                 shuffle=True)\n",
    "\n",
    "test_set = train_datagen.flow_from_directory(train_path,\n",
    "                                            target_size = (200, 200),\n",
    "                                            batch_size = 16,\n",
    "                                            class_mode = 'categorical',\n",
    "                                           subset = 'validation')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1728aa",
   "metadata": {},
   "source": [
    "# Building the VGG16 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f51d0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vimalkumar/Documents/Python_new_1/cde_venv/lib/python3.8/site-packages/keras/engine/training.py:1972: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n",
      "/home/vimalkumar/Documents/Python_new_1/cde_venv/lib/python3.8/site-packages/PIL/Image.py:962: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
      "  warnings.warn(\n",
      "2022-02-19 13:20:07.426896: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212/212 [==============================] - 559s 3s/step - loss: 0.2684 - accuracy: 0.9170 - val_loss: 0.1756 - val_accuracy: 0.9431\n"
     ]
    }
   ],
   "source": [
    "r = model.fit_generator(\n",
    "  training_set,\n",
    "  validation_data=test_set,\n",
    "  epochs=1,\n",
    "  #steps_per_epoch=len(training_set),\n",
    "  #validation_steps=len(test_set)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1e64e9",
   "metadata": {},
   "source": [
    "# Training Accuracy = 91.70%\n",
    "# Testing Accuracy = 94.31%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3232c2c",
   "metadata": {},
   "source": [
    "# Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "28f52f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cooldrinks_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df4b449",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
